import os
import asyncio
from typing import List, Dict
from datetime import datetime
import json

from content.scrapers import AmazonMoversScraper, GoogleTrendsScraper
from content.generator import ContentGenerator, ArticleDraft
from mycelium.constitution import RootSystem

class TrendHunterAgent:
    """
    Frictionless affiliate content machine.
    Finds trending products, writes articles, tracks performance.
    """
    
    def __init__(self, serpapi_key: str = None):
        self.serpapi_key = serpapi_key or os.getenv("SERPAPI_KEY") or os.getenv("SERPAPI_API_KEY")
        self.amazon = AmazonMoversScraper()
        self.trends = GoogleTrendsScraper(self.serpapi_key) if self.serpapi_key else None
        self.generator = ContentGenerator()
        self.root = RootSystem()
        
        self.discovered_products: List = []
        self.generated_articles: List[ArticleDraft] = []
        self.daily_earnings_estimate = 0.0
    
    async def run_hunt_cycle(self):
        """
        One complete hunt: scrape → analyze → generate → report.
        """
        print(f"\n🔍 TREND HUNT: {datetime.now().strftime('%H:%M:%S')}")
        print("-" * 50)
        
        async with self.amazon:
            # Scrape all Amazon categories
            all_products = await self.amazon.scrape_all_categories()
            
            total_found = sum(len(products) for products in all_products.values())
            print(f"📦 Scraped {total_found} products from {len(all_products)} categories")
            
            # Cross-reference with Google Trends (if SerpAPI key available)
            trending_products = []
            
            if self.trends:
                for category, products in all_products.items():
                    for product in products[:3]:  # Top 3 per category
                        trend_data = await self.trends.get_trending_searches(product.title)
                        if trend_data.get("interest_score", 0) > 70:
                            trending_products.append(product)
                            print(f"  📈 {product.title[:50]}... (Score: {trend_data['interest_score']})")
            else:
                # No SerpAPI - use Amazon data only
                trending_products = [
                    p for products in all_products.values() 
                    for p in products if p.trend_velocity == "High"
                ][:10]
                print(f"  📊 Using Amazon velocity data (no SerpAPI trends)")
            
            # Generate articles for top opportunities
            if trending_products:
                article = self.generator.generate_article(trending_products[:5], "roundup")
                self.generated_articles.append(article)
                
                print(f"\n✍️ ARTICLE GENERATED:")
                print(f"   Title: {article.title[:60]}...")
                print(f"   Products: {len(article.products)}")
                print(f"   Est. Commission: ${article.estimated_commission:.2f}/month")
                print(f"   SEO Keywords: {', '.join(article.seo_keywords[:3])}")
                
                # Save article
                self._save_article(article)
                
                # Update earnings estimate
                self.daily_earnings_estimate += article.estimated_commission / 30
        
        self._log_hunt_results(total_found, len(trending_products))
        
        return {
            "products_found": total_found,
            "trending_identified": len(trending_products),
            "articles_generated": len(self.generated_articles),
            "daily_earnings_estimate": self.daily_earnings_estimate
        }
    
    def _save_article(self, article: ArticleDraft):
        """Save article to disk for you to publish."""
        filename = f"data/content/{article.created_at.strftime('%Y%m%d_%H%M')}_{article.title[:30].replace(' ', '_')}.md"
        
        # Ensure directory exists
        os.makedirs("data/content", exist_ok=True)
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"# {article.title}\n\n")
            f.write(f"*Generated by Trend Hunter Agent*\n")
            f.write(f"*Potential monthly commission: ${article.estimated_commission:.2f}*\n\n")
            f.write(article.content)
            f.write(f"\n\n---\n\n**Affiliate Links to Add:**\n")
            for link in article.affiliate_links:
                f.write(f"- {link}\n")
            f.write(f"\n**SEO Keywords:** {', '.join(article.seo_keywords)}\n")
        
        print(f"   💾 Saved to: {filename}")
    
    def _log_hunt_results(self, total: int, trending: int):
        """Log for performance tracking."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "products_scraped": total,
            "trending_identified": trending,
            "articles_generated": len(self.generated_articles),
            "earnings_estimate": self.daily_earnings_estimate
        }
        
        with open("data/content/hunt_log.jsonl", "a") as f:
            f.write(json.dumps(log_entry) + "\n")
    
    async def run_autonomous(self, interval_hours: int = 6):
        """
        Continuous hunting mode.
        Runs every 6 hours (4x per day).
        """
        print("🤖 TREND HUNTER AUTONOMOUS MODE")
        print(f"   Hunting every {interval_hours} hours")
        print(f"   SerpAPI: {'✅ Active' if self.serpapi_key else '⚠️ Amazon-only mode'}")
        print("   Press Ctrl+C to stop\n")
        
        try:
            while True:
                await self.run_hunt_cycle()
                
                print(f"\n⏰ Next hunt in {interval_hours} hours...")
                print(f"   Articles ready: {len(self.generated_articles)}")
                print(f"   Est. daily earnings: ${self.daily_earnings_estimate:.2f}")
                await asyncio.sleep(interval_hours * 3600)
                
        except KeyboardInterrupt:
            print("\n\n🛑 Trend Hunter entering dormancy")
            self._generate_final_report()
    
    def _generate_final_report(self):
        """Summary of hunting session."""
        print(f"\n📊 HUNT SESSION COMPLETE")
        print(f"   Total articles: {len(self.generated_articles)}")
        print(f"   Est. monthly earnings: ${self.daily_earnings_estimate * 30:.2f}")
        print(f"   Check data/content/ for all articles")
    
    def get_status(self) -> Dict:
        return {
            "articles_ready": len(self.generated_articles),
            "products_discovered": len(self.discovered_products),
            "daily_earnings_estimate": self.daily_earnings_estimate,
            "serpapi_active": bool(self.serpapi_key),
            "last_hunt": datetime.now().isoformat()
        }
